{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSHbEpT9/jvRdpeaQsJrwv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YounSooKimTech/NLP_Power/blob/main/Enron_preprocessing_N_tokens.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 771
        },
        "id": "NBD5N-S5bT3L",
        "outputId": "232e7c22-452f-4575-d72e-3640314f5f53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from opendatasets) (8.1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from opendatasets) (4.65.0)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (from opendatasets) (1.5.13)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (1.26.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.27.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2022.12.7)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (2.0.12)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n",
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: kim4321\n",
            "Your Kaggle Key: ··········\n",
            "Downloading enron-email-dataset.zip to ./enron-email-dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 358M/358M [00:03<00:00, 94.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0cd0e7b68e3e>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://www.kaggle.com/datasets/wcukierski/enron-email-dataset'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/opendatasets/__init__.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(dataset_id_or_url, data_dir, force, dry_run, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Check for a Kaggle dataset URL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_kaggle_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_id_or_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdownload_kaggle_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_id_or_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdry_run\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdry_run\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Check for Google Drive URL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/opendatasets/utils/kaggle_api.py\u001b[0m in \u001b[0;36mdownload_kaggle_dataset\u001b[0;34m(dataset_url, data_dir, force, dry_run)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Could not delete zip file, got'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             api.dataset_download_files(\n\u001b[0m\u001b[1;32m     66\u001b[0m                 \u001b[0mdataset_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0mtarget_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kaggle/api/kaggle_api_extended.py\u001b[0m in \u001b[0;36mdataset_download_files\u001b[0;34m(self, dataset, path, force, quiet, unzip)\u001b[0m\n\u001b[1;32m   1244\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutfile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1246\u001b[0;31m                         \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meffective_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1247\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBadZipFile\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m                     raise ValueError(\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36mextractall\u001b[0;34m(self, path, members, pwd)\u001b[0m\n\u001b[1;32m   1645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1646\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mzipinfo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmembers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1647\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_member\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzipinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m_extract_member\u001b[0;34m(self, member, targetpath, pwd)\u001b[0m\n\u001b[1;32m   1700\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmember\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpwd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1701\u001b[0m              \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargetpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1702\u001b[0;31m             \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtargetpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mcopyfileobj\u001b[0;34m(fsrc, fdst, length)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mfdst_write\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfsrc_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eof\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_readbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m_read1\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mZIP_DEFLATED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMIN_READ_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decompressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1004\u001b[0m             self._eof = (self._decompressor.eof or\n\u001b[1;32m   1005\u001b[0m                          \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress_left\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "! pip install opendatasets\n",
        "import opendatasets as od\n",
        "import pandas\n",
        "import re  \n",
        "\n",
        "od.download('https://www.kaggle.com/datasets/wcukierski/enron-email-dataset')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get the dataset"
      ],
      "metadata": {
        "id": "i6MMD676lCHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/enron-email-dataset/emails.csv')\n"
      ],
      "metadata": {
        "id": "PXHd9lgebhgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use the library to parse the dataset\n",
        "import email\n",
        "import pandas as pd\n",
        "from email import utils\n",
        "\n",
        "def extract_email_info(email_string):\n",
        "    email_message = email.message_from_string(email_string)\n",
        "    sender = email_message['From']\n",
        "    receiver = email_message[\"To\"]\n",
        "    n_receivers = len(utils.getaddresses([email_message['To']]))\n",
        "    CC = email_message[\"CC\"]\n",
        "    subject = email_message['Subject']\n",
        "    date = email_message['Date']\n",
        "    # Extract email body\n",
        "    body = \"\"\n",
        "    if email_message.is_multipart():\n",
        "        for part in email_message.get_payload():\n",
        "            if part.get_content_type() == 'text/plain':\n",
        "                body = part.get_payload()\n",
        "                break\n",
        "    else:\n",
        "        body = email_message.get_payload()\n",
        "    return sender, receiver, n_receivers, CC, subject, body, date\n",
        "\n",
        "df[['Sender',\"Receiver\", 'n_receivers', 'CC', 'Subject', 'Body', \"Date\"]] = df['message'].apply(extract_email_info).apply(pd.Series)"
      ],
      "metadata": {
        "id": "EP5aTnD9bnLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"parsed_Enron_df.csv\", index = False)\n",
        "\n",
        "from google.colab import files\n",
        "files.download('parsed_Enron_df.csv') "
      ],
      "metadata": {
        "id": "cgO8p0aEiD82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# got the domain for the emails\n",
        "df['Sender_domain'] = df['Sender'].str.split('@').str[1]\n",
        "df['Receiver_domain'] = df['Receiver'].str.split('@').str[1]\n",
        "\n",
        "# 1st filter: n_receiver=1, n_sender=1, no CC, use enron.com\n",
        " \n",
        "filtered_df = df[(df[\"n_receivers\"]== 1) & (df.CC.isna()) & (df.Sender_domain\t== \"enron.com\") & (df.Receiver_domain\t== \"enron.com\")]\n",
        "\n",
        "# additional filtering based on value_counts\n",
        "filtered_df = filtered_df[(filtered_df.Sender != \"enron.announcements@enron.com\") & (filtered_df.Receiver != \"all.worldwide@enron.com\") ]\n"
      ],
      "metadata": {
        "id": "kJ3CtWzejoVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# got the domain for the emails\n",
        "df['Sender_domain'] = df['Sender'].str.split('@').str[1]\n",
        "df['Receiver_domain'] = df['Receiver'].str.split('@').str[1]\n",
        "\n",
        "# 1st filter: n_receiver=1, n_sender=1, no CC, use enron.com\n",
        " \n",
        "filtered_df = df[(df[\"n_receivers\"]== 1) & (df.CC.isna()) & (df.Sender_domain\t== \"enron.com\") & (df.Receiver_domain\t== \"enron.com\")]\n",
        "\n",
        "# additional filtering based on value_counts\n",
        "filtered_df = filtered_df[(filtered_df.Sender != \"enron.announcements@enron.com\") & (filtered_df.Receiver != \"all.worldwide@enron.com\") ]\n"
      ],
      "metadata": {
        "id": "a6WybFGJjz3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove the original mail for reply and forwarded by\n",
        "\n",
        "filtered_df['content'] = filtered_df['Body'].str.split(\"---------------------- Forwarded by\").str[0] \n",
        "filtered_df[\"content\"] = filtered_df[\"content\"].str.split(\"----- Forwarded by\").str[0]\n",
        "filtered_df[\"content\"] = filtered_df[\"content\"].str.split(\"-----Original\").str[0]\n",
        "\n",
        "filtered_df[\"content\"] = filtered_df.content.replace(\"\\n\", \"\")\n",
        "filtered_df[\"content\"] = filtered_df[\"content\"].str.split(\"\\n\\n\\n\\n\").str[0]\n",
        "\n",
        "url_pattern = re.compile(r\"http\\S+|www\\S+\")\n",
        "filtered_df['content'] = filtered_df['content'].str.replace(url_pattern, '')\n",
        "\n",
        "\n",
        "import re\n",
        "\n",
        "filtered_df['content']= filtered_df[\"content\"].replace(r\"Monthly Basis Swap\", \"\",  regex=True)\n",
        "filtered_df['content'] = filtered_df[\"content\"].replace(r\"Daily Basis Swap\", \"\",  regex=True)\n",
        "filtered_df['content'] = filtered_df[\"content\"].replace(r\"Daily for Monthly Index Swap\", \"\",  regex=True)\n",
        "filtered_df['content'] = filtered_df[\"content\"].replace(r\"Fixed Price Physical\", \"\",   regex=True)\n",
        "filtered_df['content'] = filtered_df[\"content\"].replace(r\"Physical Index\", \"\",   regex=True)\n",
        "filtered_df['content'] = filtered_df[\"content\"].replace(r\"Fixed Price Swap\", \"\", regex=True)"
      ],
      "metadata": {
        "id": "qiv2cGAnj4wJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the time zone\n",
        "\n",
        "df['timezone'] = df['Date'].apply(lambda x: re.search(r'\\((.*?)\\)', x).group(1))\n",
        "df['timezone'].unique()\n",
        "\n",
        "df.timezone.value_counts()\n",
        "\n",
        "# Pacific Standard Time (PST) or Pacific Daylight Time (PDT)"
      ],
      "metadata": {
        "id": "eDFrR8cU0ppj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_df.head(2)"
      ],
      "metadata": {
        "id": "rl2-18dTkN6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "from dateutil import parser, tz\n",
        "\n",
        "# Define the date format\n",
        "date_format = \"%a, %d %b %Y %H:%M:%S %z (%Z)\"\n",
        "\n",
        "# Define the timezone to convert to\n",
        "timezone = tz.UTC\n",
        "\n",
        "# Define a function to convert a date string to a datetime object in UTC timezone\n",
        "def convert_to_utc(date_string):\n",
        "    dt_local = parser.parse(date_string)\n",
        "    dt_utc = dt_local.astimezone(timezone)\n",
        "    return dt_utc\n",
        "\n",
        "# Apply the function to the date column\n",
        "filtered_df[\"Date_UTC\"] = filtered_df[\"Date\"].apply(convert_to_utc)\n",
        "\n",
        "# Filter the DataFrame to get dates before 2001-05-01\n",
        "filtered_df = filtered_df[filtered_df[\"Date_UTC\"] < datetime(2001, 5, 1, tzinfo=timezone)]\n"
      ],
      "metadata": {
        "id": "X5K2qGiHldDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_df.tail(10)"
      ],
      "metadata": {
        "id": "fv6XdbVJlrAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# merge with status dataset"
      ],
      "metadata": {
        "id": "ae2_tHWTdg3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# status.info()\n",
        "\n",
        "status = pd.read_csv('https://raw.githubusercontent.com/burgersmoke/enron-formality/master/enron_employee_positions/diesner_employee_email_positions.csv', names=[\"Name\", \"Email\", \"Position\", \"Rank\"])\n",
        "status_wt_na = status.dropna()\n",
        "print(status.shape, status_wt_na.shape)\n",
        "\n",
        "# Assuming that \"status\" is a Pandas DataFrame containing columns \"Position\" and \"Rank\"\n",
        "rank_counts = status['Position'].value_counts(normalize=False).reset_index()\n",
        "rank_counts.columns = ['Position', 'Num_Employees']\n",
        "\n",
        "status = status.merge(rank_counts, on='Position', how='left')\n",
        "\n",
        "# Print the updated \"status\" DataFrame with new columns\n",
        "status = status.dropna(subset=[\"Position\", \"Rank\", \"Num_Employees\"])\n",
        "status_unique = status[[\"Position\", \"Rank\", \"Num_Employees\"]].drop_duplicates()\n",
        "\n",
        "status_unique.Num_Employees = status_unique.Num_Employees.round(0).astype(int)\n",
        "\n",
        "# Print the unique values of Position, Rank, and Num_Employees\n",
        "status_unique.sort_values(by=\"Rank\", ascending=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "syTkwaxebJsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "status = pd.read_csv('https://raw.githubusercontent.com/burgersmoke/enron-formality/master/enron_employee_positions/diesner_employee_email_positions.csv', names=[\"Name\", \"Email\", \"Position\", \"Rank\"])\n",
        "\n",
        "\n",
        "status = status.dropna(subset=[\"Position\"])\n",
        "status.Position.value_counts()\n",
        "\n",
        "\n",
        "print(status.Position.value_counts())"
      ],
      "metadata": {
        "id": "dKqOVq70dofL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = pd.merge(status, filtered_df, left_on='Email',right_on=\"Sender\", how='inner', suffixes=(\"Sender_\", \"sender_\"))\n",
        "merged_df = merged_df.rename(columns={'Name':'Sender_Name',\n",
        "                                      \"Email\":\"Sender_Email\",\n",
        "                                      \"Position\":\"Sender_Position\",\n",
        "                                      \"Rank\":\"Sender_Rank\"})"
      ],
      "metadata": {
        "id": "4uQs_e55d152"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = pd.merge(status, merged_df, left_on='Email',right_on=\"Receiver\", how='inner')\n",
        "merged_df = merged_df.rename(columns={'Name':'Receiver_Name',\n",
        "                                      \"Email\":\"Receiver_Email\",\n",
        "                                      \"Position\":\"Receiver_Position\",\n",
        "                                      \"Rank\":\"Receiver_Rank\"})\n",
        "merged_df.head()"
      ],
      "metadata": {
        "id": "In6XnQald38q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.info()"
      ],
      "metadata": {
        "id": "JcWRtjRlmNuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = merged_df.drop([\"Sender\", \"Receiver\", \"n_receivers\", \"CC\", \"Sender_domain\", \"Receiver_domain\"], axis=1)"
      ],
      "metadata": {
        "id": "HOFEUFkTd9GL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df[\"Sender-Receiver\"] = merged_df[\"Sender_Rank\"] - merged_df[\"Receiver_Rank\"]\n",
        "merged_df.head()\n",
        "\n",
        "def get_direction(x):\n",
        "    if x == 0:\n",
        "        return \"same\"\n",
        "    elif x < 0:\n",
        "        return \"upward\"\n",
        "    else:\n",
        "        return \"downward\"\n",
        "    \n",
        "merged_df[\"direction\"] = merged_df[\"Sender-Receiver\"].apply(get_direction)\n"
      ],
      "metadata": {
        "id": "hz4KeL4Befs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.info()\n",
        "\n",
        "merged_df.direction.value_counts()"
      ],
      "metadata": {
        "id": "zL53ADRweMgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.sample(1)"
      ],
      "metadata": {
        "id": "-6KGOJ3DbPEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df_new = merged_df.drop_duplicates(subset=['Sender_Email', 'Receiver_Email', 'Subject'], keep='first')\n"
      ],
      "metadata": {
        "id": "wUCLulPpnaHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.shape)\n",
        "print(merged_df.shape)\n",
        "print(merged_df_new.shape)"
      ],
      "metadata": {
        "id": "g1_ViSb5nIut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "text = merged_df_new[merged_df_new['Sender-Receiver'] == -6][\"content\"].sample(1)\n",
        "print(text.replace('\\n', '\\\\n'))\n"
      ],
      "metadata": {
        "id": "9G38h3zFqEiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df_new.to_csv(\"Enron_merged_df.csv\", index = False)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"Enron_merged_df.csv\") "
      ],
      "metadata": {
        "id": "Cyl2hPXPnb2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "DePhr-kEeVNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenization\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# initialize the Potter stemmer\n",
        "porter = PorterStemmer()\n",
        "\n",
        "# define a function to tokenize and stem a text string\n",
        "def tokenize_and_stem(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    stems = [porter.stem(token) for token in tokens]\n",
        "    return stems\n",
        "\n",
        "# apply the tokenize_and_stem function to the content column\n",
        "merged_df_new['tokens'] = merged_df_new['content'].apply(tokenize_and_stem)\n"
      ],
      "metadata": {
        "id": "Lm0CXpqXeWyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# initialize the stop words and punctuation\n",
        "stop_words = set(stopwords.words('english'))\n",
        "punctuation = set(string.punctuation)\n",
        "\n",
        "# define a function to clean a single token\n",
        "def clean_token(token):\n",
        "    # remove punctuation and numbers\n",
        "    token = ''.join([char for char in token if char not in punctuation and not char.isdigit()])\n",
        "    # convert to lowercase\n",
        "    token = token.lower()\n",
        "    # remove stopwords\n",
        "    if token not in stop_words:\n",
        "        return token\n",
        "\n",
        "# define a function to clean a list of tokens\n",
        "def clean_tokens(tokens):\n",
        "    cleaned_tokens = []\n",
        "    for token in tokens:\n",
        "        cleaned_token = clean_token(token)\n",
        "        if cleaned_token:\n",
        "            cleaned_tokens.append(cleaned_token)\n",
        "    return cleaned_tokens\n",
        "\n",
        "# apply the clean_tokens function to the \"tokens\" column and store the cleaned tokens in a new \"cleaned_tokens\" column\n",
        "merged_df_new['cleaned_tokens'] = merged_df_new['tokens'].apply(clean_tokens)\n",
        "merged_df_new.head()"
      ],
      "metadata": {
        "id": "ti-Y6A0qe7JJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis of tokens by direction"
      ],
      "metadata": {
        "id": "pvQq4H3efAon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df_new.to_csv(\"Enron_filtered_dropna.csv\", na_rep=\"\", index=False)"
      ],
      "metadata": {
        "id": "HyOldYRUgYkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(merged_df.shape, merged_df_new.shape)"
      ],
      "metadata": {
        "id": "tJw4T3rs7cRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# print(stop_words)\n",
        "\n",
        "word_check = \"could\"\n",
        "if word_check in stop_words:\n",
        "    print(word_check, \"is stopwords\")\n",
        "else:\n",
        "    print(\"No\", word_check, \"is not in stopwords\" )\n"
      ],
      "metadata": {
        "id": "pMrj2eD3ftWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(merged_df.groupby(\"direction\")[\"tokens\"].apply(lambda x: x.apply(len).mean()))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "print(merged_df.groupby(\"direction\")[\"cleaned_tokens\"].apply(lambda x: x.apply(len).mean()))\n"
      ],
      "metadata": {
        "id": "KeTUHlARfEbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df_new.info()"
      ],
      "metadata": {
        "id": "d17pmAfFmtK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(merged_df_new.groupby(\"direction\")[\"tokens\"].apply(lambda x: x.apply(len).mean()))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "print(merged_df_new.groupby(\"direction\")[\"cleaned_tokens\"].apply(lambda x: x.apply(len).mean()))\n"
      ],
      "metadata": {
        "id": "1kHWIgrfiXhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df_new[\"n_tokens\"] = merged_df_new[\"tokens\"].apply(len)\n",
        "merged_df_new[\"n_cleaned_tokens\"] = merged_df_new[\"cleaned_tokens\"].apply(len)"
      ],
      "metadata": {
        "id": "khr7h2HtoV1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df_new.head()"
      ],
      "metadata": {
        "id": "llA3jZYKqCIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df_new.groupby(\"direction\")[\"n_tokens\"].mean()"
      ],
      "metadata": {
        "id": "09O29grOpmx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "\n",
        "# fit an ANOVA model with the 'values' column as the dependent variable and the 'group' column as the independent variable\n",
        "model = ols('n_cleaned_tokens ~ C(direction)', data=merged_df_new).fit()\n",
        "\n",
        "# perform an ANOVA test\n",
        "anova_table = sm.stats.anova_lm(model, typ=2)\n",
        "\n",
        "# print the ANOVA table\n",
        "print(anova_table)\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "# perform Tukey's HSD test\n",
        "tukey_results = pairwise_tukeyhsd(merged_df_new['n_cleaned_tokens'], merged_df_new['direction'])\n",
        "\n",
        "# print the Tukey's HSD results\n",
        "print(tukey_results)\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "means =merged_df_new.groupby('direction')['n_cleaned_tokens'].mean()\n",
        "plt.bar(means.index, means.values)\n",
        "plt.title('Mean values by category')\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Mean value')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HS51OX2Znv_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(merged_df.groupby(\"Sender-Receiver\")[\"tokens\"].apply(lambda x: x.apply(len).mean()))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "print(merged_df.groupby(\"Sender-Receiver\")[\"cleaned_tokens\"].apply(lambda x: x.apply(len).mean()))"
      ],
      "metadata": {
        "id": "JvVj1vkLfMwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZuL-7GeOgQVW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}